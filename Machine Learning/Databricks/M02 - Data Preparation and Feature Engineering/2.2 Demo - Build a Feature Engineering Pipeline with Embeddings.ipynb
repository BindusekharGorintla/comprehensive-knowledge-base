{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5b88077-6eec-41bd-bc6d-0ff96dd37481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29b938ee-c6ff-4f90-b865-ea7b0a619b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demo - Build a Feature Engineering Pipeline with Embeddings\n",
    "\n",
    "In this demo, we will build a feature engineering pipeline that performs data loading, imputation, transformation, and embedding generation for categorical features. The pipeline will be applied to training and testing datasets, ensuring consistency in data preprocessing. Finally, we will save the pipeline for future reuse, allowing efficient and reproducible data preparation for machine learning.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "*By the end of this demo, you will be able to:*\n",
    "\n",
    "* Build a structured feature engineering pipeline that includes multiple preprocessing steps.\n",
    "* Create a pipeline with tasks for data imputation and numerical feature scaling.\n",
    "* Generate embeddings for categorical features to represent categorical data effectively.\n",
    "* Assemble transformed numerical and embedded categorical features into a single feature vector.\n",
    "* Apply the feature engineering pipeline to both training and test datasets.\n",
    "* Display the results of the transformation.\n",
    "* Save a data preparation and feature engineering pipeline to Unity Catalog for potential future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d9df531-59fe-4f98-9caa-894f90470bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a09a76d-007e-4f80-a670-e0f4e54ebf86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e300dc34-d9aa-4551-ac5b-f2a209f7c6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89df87e7-f385-48d1-ae3b-d217981655e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "940846bb-8ae5-4b72-b6f8-a7fe120a6510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains **variables such as your username, catalog name, schema name, working directory, and dataset locations**. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff7f010-eeda-48be-baf1-5b68e752268e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93b91db1-930f-4b73-a6ef-ac722c8369a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Preparation\n",
    "\n",
    "Before constructing the feature engineering pipeline, we need to ensure the dataset is consistent and properly formatted. This includes handling data types, addressing missing values, and preparing the dataset for further transformations. The `Telco Customer Churn` dataset will be used for this process.\n",
    "\n",
    "**Steps in Data Preparation:**\n",
    "1. Load the dataset into a Spark DataFrame.\n",
    "1. Split the dataset into training and testing sets.\n",
    "1. Convert Integer and Boolean columns to Double to ensure compatibility with Spark ML.\n",
    "1. Handle missing values by identifying and imputing them in:\n",
    "    - Numeric columns\n",
    "    - String columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a48bbfaa-89a2-4e20-9148-b42455dae415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Loading the Dataset\n",
    "We start by loading the dataset from the specified file path using Spark. Note that we will use `.option(\"nullValue\", \" \")` since this particular CSV has empty string values that need to be read as null. This will allow for proper type casting of the `TotalCharges` column. \n",
    "\n",
    "> This step ensures that only relevant columns are included for feature engineering and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb9bd86d-4946-4754-8a84-4384a0aac84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn-missing' # CSV file name\n",
    "dataset_path = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "telco_df = spark.read.option(\"nullValue\", \" \").csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# Select columns of interest\n",
    "telco_df = telco_df.select(\"gender\", \"SeniorCitizen\", \"Partner\", \"tenure\", \"InternetService\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\", \"TotalCharges\", \"Churn\")\n",
    "\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4084cca9-d317-4cdd-ae02-6c3e9f10dc89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Splitting the Dataset into Training and Testing Sets\n",
    "Once the data has been cleaned, we split it into training and testing sets using an 80-20 split.\n",
    "> Since telco_df is a PySpark DataFrame, we will use `randomSplit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffae4b86-56b2-4080-9848-e6d72f2f2a81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = telco_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1511a32-78cb-470b-9126-7dd48ef72315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transforming the Dataset\n",
    "To ensure that all numerical and categorical features are compatible with machine learning algorithms, we perform several transformations.\n",
    "\n",
    "**Convert Integer and Boolean Columns to Double**\n",
    "\n",
    "- Many machine learning algorithms require numeric input, so we convert all **integer and boolean** columns to **double**.\n",
    "    > This ensures numerical consistency in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e9434eb-0129-4dd5-b946-b982f5f3c691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, BooleanType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "\n",
    "# Get a list of integer & boolean columns\n",
    "integer_cols = [column.name for column in train_df.schema.fields if (column.dataType == IntegerType() or column.dataType == BooleanType())]\n",
    "\n",
    "# Loop through integer columns to cast each one to double\n",
    "for column in integer_cols:\n",
    "    train_df = train_df.withColumn(column, col(column).cast(\"double\"))\n",
    "    test_df = test_df.withColumn(column, col(column).cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8402fe87-39d7-4e22-8651-93b78963660b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Identifying Missing Values**\n",
    "\n",
    "Handling missing data is crucial to prevent errors and bias in machine learning models. We first check for missing values in numerical and categorical columns.\n",
    "\n",
    "- **Find Numeric Columns with Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d62b296-f28c-4bff-8e1d-2e5f5e34cbad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "# Identify numeric columns\n",
    "num_cols = [c.name for c in train_df.schema.fields if c.dataType == DoubleType()]\n",
    "\n",
    "# Count missing values in numeric columns\n",
    "num_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in num_cols]\n",
    "row_dict_num = train_df.select(num_missing_values_logic).first().asDict()\n",
    "num_missing_cols = [column for column in row_dict_num if row_dict_num[column] > 0]\n",
    "\n",
    "print(f\"Numeric columns with missing values: {num_missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb2733c9-a8b2-4a19-a5d6-fd23454baab5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Find String Columns with Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b6b23ca-6072-4675-9160-a3c710f577c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify string columns\n",
    "string_cols = [c.name for c in train_df.schema.fields if c.dataType == StringType()]\n",
    "\n",
    "# Count missing values in string columns\n",
    "string_missing_values_logic = [count(when(col(column).isNull(), column)).alias(column) for column in string_cols]\n",
    "row_dict_string = train_df.select(string_missing_values_logic).first().asDict()\n",
    "string_missing_cols = [column for column in row_dict_string if row_dict_string[column] > 0]\n",
    "\n",
    "print(f\"String columns with missing values: {string_missing_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "698bd05c-7047-495b-94c5-31c34e806610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating a Feature Engineering Pipeline\n",
    "\n",
    "To efficiently preprocess and transform data for machine learning, we construct a **Spark ML pipeline**. This pipeline automates key preprocessing steps, ensuring **consistency** and **reproducibility** in data preparation. The pipeline processes the **Telco Customer Churn** dataset by performing:\n",
    "\n",
    "**Key Feature Engineering Steps:**\n",
    "- **Generating Embeddings for Categorical Features**  \n",
    "  - Instead of traditional encoding techniques, we generate **dense vector representations** using SparkML's **Word2Vec**.  \n",
    "  - These embeddings capture **semantic relationships** between categories, improving model performance.  \n",
    "  > For example, we aim to capture the relationship between a __senior citizen with fiber optic internet__ and one with DSL. By combining categorical columns into a single sequence before embedding, we allow the model to learn cross-column patterns.\n",
    "\n",
    "- **Handling Missing Values**  \n",
    "  - Missing values in **numerical columns** (e.g., `tenure`, `TotalCharges`) are imputed using the **mean strategy** to ensure completeness.  \n",
    "  - Missing **categorical values** are handled by replacing them with `\"unknown\"` before embedding.\n",
    "\n",
    "- **Standardizing Numerical Features**  \n",
    "  - SparkML's `VectorAssembler` **combines imputed numerical columns** into a single feature vector.  \n",
    "  - SparkML's `StandardScaler` **standardizes numerical values**, reducing sensitivity to outliers.  \n",
    "\n",
    "- **Combining Features into a Final Vector**  \n",
    "  - The **scaled numerical features** and the **single categorical embedding vector** are **combined** into one final feature vector.  \n",
    "  - This results in a **structured and uniform format** for downstream machine learning models.\n",
    "\n",
    "- **Encapsulating Steps into a Pipeline**  \n",
    "  - All preprocessing steps are included in a **Spark ML pipeline**, making the transformations **modular, reusable, and production-ready**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "178a27c5-456a-425b-8c85-b417d45104af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Generating Embeddings for Categorical Features\n",
    "\n",
    "To improve model performance, we create **embedding vectors** for categorical columns using **Word2Vec**. These embeddings capture complex relationships between different categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74cec6e6-ebe5-4804-ab0b-3ea9e5ce9890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> \uD83D\uDCA1 **Embedding Strategy: One Combined vs. Per-Column Embeddings**\n",
    "\n",
    "In this demo, we generate **a single embedding vector** by combining all categorical columns into one sequence.  \n",
    "This approach captures **interactions across multiple categorical features** (e.g., how contract type and internet service may co-occur).\n",
    "\n",
    "However, there are two common strategies when using embeddings:\n",
    "\n",
    "| Strategy | Pros | Cons |\n",
    "|---------|------|------|\n",
    "| **Combined Embedding (used here)** | Captures relationships across columns; <br> Smaller feature set | Less interpretable; <br>May dilute column-specific meaning |\n",
    "| **Per-Column Embeddings** | Clear semantic meaning per column;<br> Easier to debug | Higher dimensionality; <br>Doesn’t capture cross-column interactions |\n",
    "\n",
    "> There’s no one-size-fits-all solution. Combined embeddings are great for models that benefit from feature interactions, while per-column embeddings may be more suitable when interpretability is important.\n",
    "\n",
    "Feel free to experiment with both approaches and choose based on your dataset and modeling goals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a0caca3-19d7-4688-8f23-fa95e1486de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import split, concat_ws, col, when\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def generate_categorical_embeddings(train_df, test_df, categorical_cols, vector_size=5):\n",
    "    \"\"\"\n",
    "    Generate embeddings for categorical columns using Word2Vec trained on the training DataFrame,\n",
    "    and apply the same model to both train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df (DataFrame): Training Spark DataFrame\n",
    "    - test_df (DataFrame): Testing Spark DataFrame\n",
    "    - categorical_cols (list): List of categorical column names\n",
    "    - vector_size (int): Size of the embedding vectors\n",
    "\n",
    "    Returns:\n",
    "    - Transformed train_df and test_df with embeddings as a single vector column\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace NULL categorical values with \"unknown\"\n",
    "    for col_name in categorical_cols:\n",
    "        train_df = train_df.withColumn(col_name, when(col(col_name).isNull(), \"unknown\").otherwise(col(col_name)))\n",
    "        test_df = test_df.withColumn(col_name, when(col(col_name).isNull(), \"unknown\").otherwise(col(col_name)))\n",
    "\n",
    "    # Combine all categorical columns into a single text column\n",
    "    train_df = train_df.withColumn(\"categorical_sequence\", concat_ws(\" \", *categorical_cols))\n",
    "    test_df = test_df.withColumn(\"categorical_sequence\", concat_ws(\" \", *categorical_cols))\n",
    "\n",
    "    # Tokenize categorical data\n",
    "    train_df = train_df.withColumn(\"categorical_tokens\", split(col(\"categorical_sequence\"), \" \"))\n",
    "    test_df = test_df.withColumn(\"categorical_tokens\", split(col(\"categorical_sequence\"), \" \"))\n",
    "\n",
    "    # Train Word2Vec model on training data\n",
    "    word2vec = Word2Vec(vectorSize=vector_size, minCount=0, inputCol=\"categorical_tokens\", outputCol=\"embedding_struct\")\n",
    "    model = word2vec.fit(train_df)\n",
    "\n",
    "    # Apply model to both train and test sets\n",
    "    train_df = model.transform(train_df)\n",
    "    test_df = model.transform(test_df)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e44714a-4c3f-4d0a-b6cd-edb880d5956c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Applying Embeddings to Categorical Features\n",
    "We apply the embedding function to the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a8aea3d-b7b7-4cd3-8819-4d57e271c6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define categorical columns\n",
    "categorical_columns = [\"gender\", \"Partner\", \"InternetService\", \"Contract\", \"PaperlessBilling\", \"PaymentMethod\"]\n",
    "\n",
    "# Generate embeddings for categorical columns (fit on train, transform both)\n",
    "train_df, test_df = generate_categorical_embeddings(train_df, test_df, categorical_columns)\n",
    "\n",
    "# Display result\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0418f740-48dd-4116-9c9e-ccc14b79b7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Converting Embeddings into Dense Vectors (Optional)\n",
    "\n",
    "For compatibility with Spark ML models, we ensure that embedding vectors are in `DenseVector` format.\n",
    "\n",
    "> **Note:** Spark's `Word2Vec` model already produces embeddings as `DenseVector` by default.  \n",
    "> \n",
    "> In this step, we demonstrate how to manually convert embeddings into `DenseVector` format, a useful pattern when working with transformations that may output sparse vectors (e.g., one-hot encoders or feature hashing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73b2e1ae-b75a-41f4-9fe0-201dbf04f8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the single embedding_struct column to DenseVector\n",
    "DenseVector_udf = F.udf(lambda v: DenseVector(v.values) if v else DenseVector([0.0] * 5), VectorUDT())\n",
    "train_df = train_df.withColumn(\"categorical_embedding\", DenseVector_udf(F.col(\"embedding_struct\")))\n",
    "test_df = test_df.withColumn(\"categorical_embedding\", DenseVector_udf(F.col(\"embedding_struct\")))\n",
    "\n",
    "# Drop temporary columns\n",
    "train_df = train_df.drop(\"categorical_sequence\", \"categorical_tokens\", \"embedding_struct\")\n",
    "test_df = test_df.drop(\"categorical_sequence\", \"categorical_tokens\", \"embedding_struct\")\n",
    "\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fff6574-9993-47f2-bccf-5898281ec380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Feature Engineering and Pipeline Initialization\n",
    "Now that categorical columns have been transformed into embeddings, we finalize the feature engineering steps.\n",
    "\n",
    "- **Handle Missing Values in Numerical Columns**\n",
    "  - Impute missing numerical values with the mean of each column.\n",
    "- **Standardize Numerical Features**\n",
    "  - Use `StandardScaler` to normalize numerical features, reducing sensitivity to outliers.\n",
    "- **Assemble the Final Feature Vector**\n",
    "  - Combine numerical and categorical embeddings into a single feature vector.\n",
    "- **Initializing the Spark ML Pipeline**\n",
    "  - Encapsulate all transformations into a Spark ML Pipeline for structured data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0054f16-80f8-480b-bd73-987f42e21b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define numerical columns for imputation\n",
    "numerical_cols = [\"SeniorCitizen\", \"tenure\", \"TotalCharges\"]\n",
    "\n",
    "# Impute missing numerical features\n",
    "imputer = Imputer(inputCols=numerical_cols, outputCols=[col + \"_imputed\" for col in numerical_cols])\n",
    "\n",
    "# Assemble numerical columns into a single vector\n",
    "numerical_assembler = VectorAssembler(inputCols=[col + \"_imputed\" for col in numerical_cols], outputCol=\"numerical_assembled\")\n",
    "\n",
    "# Scale numerical features to standardize values\n",
    "numerical_scaler = StandardScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "\n",
    "# Assemble all features (numerical + single embedding vector) into a single feature vector\n",
    "feature_cols = [\"numerical_scaled\", \"categorical_embedding\"]\n",
    "vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"all_features\")\n",
    "\n",
    "# Define the sequence of transformations\n",
    "stages_list = [imputer, numerical_assembler, numerical_scaler, vector_assembler]\n",
    "\n",
    "# Instantiate the pipeline\n",
    "pipeline = Pipeline(stages=stages_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d861c6bd-d270-4975-8db7-01405b9281c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Fit the Pipeline\n",
    "\n",
    "In the context of machine learning and MLflow, **`fitting`** corresponds to the process of training a machine learning model on a specified dataset. \n",
    "\n",
    "In the previous step we created a pipeline. Now, we will fit a model based on the pipeline. This pipeline will impute missing values, scale numerical columns, generate embeddings for categorical variables, and create a feature vector for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65a3417d-424e-43b1-9b1e-80fc84eea5b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**What Happens During Fitting?**\n",
    "\n",
    "When we call `.fit(train_df)`, the pipeline applies the following transformations:\n",
    "\n",
    "- **Imputation of Missing Values**\n",
    "  - The `Imputer` calculates the **mean** for numerical columns in `train_df` and replaces missing values accordingly.\n",
    "  \n",
    "- **Scaling of Numerical Features**\n",
    "  - The `StandardScaler` computes **scaling factors** based on the distribution of numerical features.\n",
    "  - These factors are applied uniformly across datasets to **normalize feature values**.\n",
    "\n",
    "- **Generating Embeddings for Categorical Variables**\n",
    "  - The **Word2Vec model** converts categorical text data into **dense vector representations**.\n",
    "  - These embeddings **capture semantic relationships** between categories.\n",
    "  - The trained embedding model is **stored** and later applied to unseen data.\n",
    "\n",
    "- **Combining Features into a Single Vector**\n",
    "  - The `VectorAssembler` consolidates:\n",
    "  \n",
    "    Machine learning models in Spark ML require **all features to be represented as a single vector**.\n",
    "    - **Scaled numerical features** - standardized values from `StandardScaler`.\n",
    "    - **Categorical embeddings** - Dense embeddings generated by Word2Vec.\n",
    "  - This results in a **final feature vector**, ready for input into machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76934694-bf7f-4677-a18e-bce55f5efb67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fit the Pipeline\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b81bd074-5523-4447-a09b-c7964e9f1ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Apply the Feature Engineering Pipeline\n",
    "\n",
    "Once the pipeline is **fitted** to the training data, it can be **applied to any dataset** using `.transform()`.  \n",
    "We apply the pipeline to both:\n",
    "- **Train Dataset (`train_df`)** → Generates **transformed training features**.\n",
    "- **Test Dataset (`test_df`)** → Ensures that the same transformations are applied consistently.\n",
    "\n",
    "The output is a **transformed dataset** with the **final feature vector** ready for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39faa226-3d87-4323-a305-fb018bcfe54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform both training_df and test_df\n",
    "train_transformed_df = pipeline_model.transform(train_df)\n",
    "test_transformed_df = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535eb0e3-9fc9-478b-b128-9cc1c2f825a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show transformed features\n",
    "train_transformed_df.select(\"all_features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae60196c-6c94-4582-b21c-ab864bfa7e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save and Reuse the Pipeline\n",
    "\n",
    "Preserving the Telco Customer Churn Prediction pipeline, encompassing the model, parameters, and metadata, is vital for maintaining reproducibility, enabling version control, and facilitating collaboration among team members. This ensures a detailed record of the machine learning workflow. In this section, we will follow these steps;\n",
    "\n",
    "1. **Save the Pipeline:** Save the pipeline model, including all relevant components, to the designated artifact storage. The saved pipeline is organized within the **`spark_pipelines`** folder for clarity.\n",
    "\n",
    "1. **Explore Loaded Pipeline Stages:** Upon loading the pipeline, inspect the stages to reveal key transformations and understand the sequence of operations applied during the pipeline's execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56c1073-0593-448f-8df5-7d412585433c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Save the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "471e8359-c9bf-47cd-90d5-d160711ee454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the pipeline model with overwrite mode\n",
    "pipeline_model.write().overwrite().save(f\"{DA.paths.working_dir}/spark_pipelines\")\n",
    "print(f\"Saved model to: {DA.paths.working_dir}/spark_pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8d21c7-3a41-44d1-bd84-a6a38621f62b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load and Use Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f949b270-30b7-42c0-a5ca-353b1d629fb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load and use the saved model\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "loaded_pipeline = PipelineModel.load(f\"{DA.paths.working_dir}/spark_pipelines\")\n",
    "\n",
    "# Show pipeline stages\n",
    "loaded_pipeline.stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7655a78-5f8f-4709-82db-108804e465d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **Using Saved Pipeline for Reuse**  \n",
    "> \n",
    "> Although we already applied the pipeline earlier in this demo, we reload the saved pipeline and apply it again here to illustrate how saved pipelines can be reused in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0d5a8a1-575f-4010-b007-5525d950ed5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the loaded pipeline to transform the test dataset\n",
    "test_transformed_df = loaded_pipeline.transform(test_df)\n",
    "display(test_transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8e1b2dd-21f2-43aa-a905-52e9cd71b122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this demo, we built a feature engineering pipeline to streamline data preparation. The pipeline handled data loading, missing value imputation, numerical feature scaling, and generated embeddings for categorical variables using `Word2Vec`. \n",
    "\n",
    "By applying the pipeline to both training and test sets, we ensured a consistent and reproducible feature transformation process. Finally, saving the pipeline allows for future reuse, enabling efficient and standardized data preprocessing for machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "620c2565-7b6a-482c-a563-225a1419e449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.2 Demo - Build a Feature Engineering Pipeline with Embeddings",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}