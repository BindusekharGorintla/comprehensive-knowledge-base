{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e914e95-05c5-42b5-97bb-1554cd253145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ead08f3-40dc-44d2-bb17-09ca8a7543f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Demo - Data Imputation and Transformation Pipeline\n",
    "\n",
    "In this demo, we'll delve into techniques such as preparing modeling data, including splitting data, handling missing values, encoding categorical features, and standardizing features. We will also discuss outlier removal and coercing columns to the correct data type. By the end, you will have a comprehensive understanding of data preparation for modeling and feature preparation.\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "By the end of this demo, you will be able to: \n",
    "\n",
    "- Coerce columns to be the correct data type based on feature or target variable type.\n",
    "- Identify and remove outliers from the modeling data.\n",
    "- Drop rows/columns that contain missing values.\n",
    "- Impute categorical missing values with the mode value.\n",
    "- Replace missing values with a specified replacement value.\n",
    "- One-hot encode categorical features.\n",
    "- Perform ordered indexing as an alternative categorical feature preparation for random forest modeling.\n",
    "- Apply pre-existing embeddings to categorical features.\n",
    "- Standardize features in a training set.\n",
    "- Split modeling data into a train-test-holdout split as part of a modeling process.\n",
    "- Split training data into cross-validation sets as part of a modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f4fd75-4eed-46d9-9fe2-367704246b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396ffcb7-0e95-4d6b-b02e-2386baf55e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **17.3.x-cpu-ml-scala2.13**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24019ac1-c366-4fa0-b42c-2f1db1f47b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the demo, run the provided classroom setup script. This script will define configuration variables necessary for the demo. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0b58ee1-3ac4-4fcc-8036-9c14ddb06882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80cba1c2-fca0-4fd4-9bde-2434bd8fb66a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "019ccdde-4f1e-41bc-b7c4-d52dbd7a1ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08963a06-497d-4b94-8268-fa74eb0344ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Data Cleaning and Imputation\n",
    "\n",
    "- Load the dataset from the specified path using Spark and read it as a DataFrame.\n",
    "\n",
    "- Drop any rows with missing values from the DataFrame using the **`dropna()`** method.\n",
    "\n",
    "- Fill any remaining missing values in the DataFrame with the 0 using the **`fillna()`** method.\n",
    "\n",
    "- Create a temporary view named as **`telco_customer_churn`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f91b13b-217b-474c-a067-b5abe1473c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset with spark\n",
    "shared_volume_name = 'telco' # From Marketplace\n",
    "csv_name = 'telco-customer-churn-noisy' # CSV file name\n",
    "dataset_path = f\"{DA.paths.datasets.telco}/{shared_volume_name}/{csv_name}.csv\" # Full path\n",
    "\n",
    "\n",
    "telco_df = spark.read.csv(dataset_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "\n",
    "# telco_df.printSchema()\n",
    "display(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb15e08a-16a8-4a00-a190-a302196ac41f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Coerce/Fix Data Types\n",
    "\n",
    "Even though most of the data types are correct let's do the following to have a better memory footprint of the dataframe in memory\n",
    "\n",
    "* Convert **`SeniorCitizen`** and **`Churn`** binary columns to boolean type.\n",
    "\n",
    "* Converting the **`tenure`** column to a long integer using **`.selectExpr`** and reordering the columns.\n",
    "\n",
    "* Using **`spark.sql`** to convert **`Partner`** , **`Dependents`**, **`PhoneService`** and **`PaperlessBilling`** columns to boolean, and reordering the columns. Then, saving the dataframe as a DELTA table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d05fa908-8afa-490f-901b-61545025ff31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType, ShortType, IntegerType\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "\n",
    "binary_columns = [\"SeniorCitizen\", \"Churn\"]\n",
    "telco_customer_churn_df = telco_df\n",
    "for column in binary_columns:\n",
    "    telco_customer_churn_df = telco_customer_churn_df.withColumn(column, col(column).cast(BooleanType()))\n",
    "\n",
    "telco_customer_churn_df.select(*binary_columns).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7619aa83-d496-4781-9609-1164dde5a679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PhoneService & PaperlessBilling to new boolean using spark.sql and re-order columns\n",
    "telco_customer_churn_df.createOrReplaceTempView(\"telco_customer_churn_temp_view\")\n",
    "\n",
    "telco_customer_casted_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        customerID,\n",
    "        BOOLEAN(Dependents),\n",
    "        BOOLEAN(Partner),\n",
    "        BOOLEAN(PhoneService),\n",
    "        BOOLEAN(PaperlessBilling),\n",
    "        * \n",
    "        EXCEPT (customerID, Dependents, Partner, PhoneService, PaperlessBilling, Churn),\n",
    "        Churn\n",
    "    FROM telco_customer_churn_temp_view\n",
    "\"\"\")\n",
    "\n",
    "telco_customer_casted_df.select(\"Dependents\",\"Partner\",\"PaperlessBilling\", \"PhoneService\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9932fa4c-4cee-4747-a11e-40f770dce4bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tenure months to Long/Integer using .selectExpr\n",
    "telco_customer_casted_df = telco_customer_casted_df.selectExpr(\"* except(tenure)\", \"cast(tenure as long) tenure\")\n",
    "telco_customer_casted_df.select(\"tenure\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5953e612-3869-4992-887d-65d80841ac16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Handling Outliers\n",
    "\n",
    "We will see how to handle outliers in column by identifying and addressing data points that fall far outside the typical range of values in a dataset. Common methods for handling outliers include removing them, filtering, transforming the data, or replacing outliers with more representative values. \n",
    "\n",
    "Follow these steps for handling outliers:\n",
    "* Create a new silver table named as **`telco_customer_full_silver`** by appending **`silver`** to the original table name and then accessing it using Spark SQL.\n",
    "\n",
    "* Filtering out outliers from the **`TotalCharges`** column by removing rows where the column value exceeds the specified cutoff value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de175454-2b46-4575-9e8c-0faa70d0b41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_name_full = \"telco_customer_full\"\n",
    "\n",
    "# [OPTIONAL] Save as DELTA table (silver)\n",
    "telco_customer_full_silver = f\"{telco_customer_name_full}_silver\"\n",
    "telco_customer_casted_df.write.mode(\"overwrite\").option(\"mergeSchema\",True).saveAsTable(telco_customer_full_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d5c1b0b-f7df-4672-84b8-2a51a597f122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Filtering out outliers from the **`TotalCharges`** column by removing rows where the column value exceeds the specified cutoff value (e.g. negative values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1b77c21-cf28-44c9-a8f1-b3e23d2687fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(telco_customer_casted_df)\n",
    "telco_customer_casted_df.select(\"TotalCharges\", \"tenure\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9706beb7-6d8f-4960-a8e6-832f752327d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Remove customers with negative TotalCharges \n",
    "TotalCharges_cutoff = 0\n",
    "\n",
    "# Use .filter method and SQL col() function\n",
    "telco_no_outliers_df = telco_customer_casted_df.filter(\\\n",
    "    (col(\"TotalCharges\") > TotalCharges_cutoff) | \\\n",
    "    (col(\"TotalCharges\").isNull())) # Keep Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "875704a7-6134-4b3c-b4d9-5c827dce78d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Removing outliers from PaymentMethod**\n",
    "* Identify the two lowest occurrence groups in the **`PaymentMethod`** column and calculating the total count and average **`MonthlyCharges`** for each group.\n",
    "\n",
    "* Removing customers from the identified low occurrence groups in the **`PaymentMethod`** column to filter out outliers.\n",
    "\n",
    "* Create a new dataframe **`telco_filtered_df`** containing the filtered data.\n",
    "\n",
    "* Comparing the count of records before and after by dividing the count of **`telco_casted_full_df`** and **`telco_no_outliers_df`** dataframe removing outliers and then materializing the resulting dataframe as a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e82ee4a9-e5cd-48e1-9ae1-185e08f7cb92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, avg\n",
    "\n",
    "\n",
    "# Identify 2 lowest group occurrences\n",
    "group_var = \"PaymentMethod\"\n",
    "stats_df = telco_no_outliers_df.groupBy(group_var) \\\n",
    "                      .agg(count(\"*\").alias(\"Total\"),\\\n",
    "                           avg(\"MonthlyCharges\").alias(\"MonthlyCharges\")) \\\n",
    "                      .orderBy(col(\"Total\").desc())\n",
    "\n",
    "# Display\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e00a919-400f-43dd-a7a9-e51cfbf9ab73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gather 2 groups with the lowest counts assuming the count threshold is below 20% of the full dataset and monthly charges < $70\n",
    "N = telco_no_outliers_df.count()  # total count\n",
    "lower_groups = [elem[group_var] if elem[group_var] is not None else \"null\" for elem in stats_df.tail(2) if elem['Total']/N < 0.2 and elem['MonthlyCharges'] < 70]\n",
    "print(f\"Removing groups: {','.join(lower_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60b4548-c2ec-4ae9-973d-63eafd2f31f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter/Remove listings from these low occurrence groups while keeping null occurrences\n",
    "telco_no_outliers_df = telco_no_outliers_df.filter( \\\n",
    "    ~col(group_var).isin(lower_groups) | \\\n",
    "    col(group_var).isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30648e5a-a6c6-4621-82cc-379e7a3a657c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count/Compare datasets before/after removing outliers\n",
    "print(f\"Count - Before: {telco_customer_casted_df.count()} / After: {telco_no_outliers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9db319c-5337-4915-a73c-047e7e332bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialize/Snap table [OPTIONAL/for instructor only]\n",
    "telco_no_outliers_df.write.mode(\"overwrite\").saveAsTable(telco_customer_full_silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "110e3f42-cbfa-411e-bee8-134e5c01292a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Handling Missing Values\n",
    "\n",
    "To Handle missing values in dataset we need to identify columns with high percentages of missing data and drops those columns. Then, we will remove rows with missing values. Numeric columns are imputed with 0, and string columns are imputed with 'N/A'. Overall, the code demonstrates a comprehensive approach to handling missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5456108d-2a3a-4271-97a8-77afaf571bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Delete Columns \n",
    "\n",
    "* Create a DataFrame called **`missing_df`** to count the missing values per column in the **`telco_no_outliers_df`** dataset.\n",
    "\n",
    "* The **`missing_df`** DataFrame is then transposed for better readability using the TransposeDF function, which allows for easier analysis of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89b11491-3506-433c-9236-fa652cb8f6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    when,\n",
    "    count,\n",
    "    concat_ws,\n",
    "    collect_list,\n",
    "    sum,\n",
    "    length,\n",
    "    trim,\n",
    "    lower,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_missing(input_df, show=True):\n",
    "    \"\"\"\n",
    "    Helper function to calculate and display missing data\n",
    "    \"\"\"\n",
    "\n",
    "    # First get count of missing values per column to get a singleton row DF\n",
    "    missing_df_ = input_df.agg(\n",
    "        *[\n",
    "            sum(\n",
    "                when(\n",
    "                    col(c).isNull()\n",
    "                    | (length(trim(col(c).cast(\"string\"))) == 0)\n",
    "                    | lower(trim(col(c).cast(\"string\"))).isin([\"none\", \"null\"]),\n",
    "                    1,\n",
    "                ).otherwise(0)\n",
    "            ).alias(c)\n",
    "            for c in input_df.columns\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Transpose for better readability\n",
    "    def TransposeDF(df, columns, pivotCol):\n",
    "        \"\"\"Helper function to transpose spark dataframe\"\"\"\n",
    "        columnsValue = list(\n",
    "            map(lambda x: str(\"'\") + str(x) + str(\"',\") + str(x), columns)\n",
    "        )\n",
    "        stackCols = \",\".join(x for x in columnsValue)\n",
    "        df_1 = df.selectExpr(\n",
    "            pivotCol, \"stack(\" + str(len(columns)) + \",\" + stackCols + \")\"\n",
    "        ).select(pivotCol, \"col0\", \"col1\")\n",
    "        final_df = (\n",
    "            df_1.groupBy(col(\"col0\"))\n",
    "            .pivot(pivotCol)\n",
    "            .agg(concat_ws(\"\", collect_list(col(\"col1\"))))\n",
    "            .withColumnRenamed(\"col0\", pivotCol)\n",
    "        )\n",
    "        return final_df\n",
    "\n",
    "    missing_df_out_T = TransposeDF(\n",
    "        spark.createDataFrame([{\"Column\": \"Number of Missing Values\"}]).join(\n",
    "            missing_df_\n",
    "        ),\n",
    "        missing_df_.columns,\n",
    "        \"Column\",\n",
    "    ).withColumn(\n",
    "        \"Number of Missing Values\", col(\"Number of Missing Values\").cast(\"long\")\n",
    "    )\n",
    "\n",
    "    if show:\n",
    "        display(missing_df_out_T.orderBy(\"Number of Missing Values\", ascending=False))\n",
    "\n",
    "    return missing_df_out_T\n",
    "\n",
    "\n",
    "missing_df = calculate_missing(telco_no_outliers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce4f02b8-2184-40a6-989d-1d2e6bb6a1bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Drop columns with more than x% of missing rows**\n",
    "\n",
    "Columns with more than 60% missing data are identified and stored in the **`to_drop_missing`** list, and these columns are subsequently dropped from the **`telco_no_outliers_df`** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58801a75-0348-4143-8797-49ff14816e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "per_thresh = 0.6  # Drop if column has more than 60% missing data\n",
    "\n",
    "N = telco_no_outliers_df.count()  # total count\n",
    "to_drop_missing = [x.asDict()['Column'] for x in missing_df.select(\"Column\").where(col(\"Number of Missing Values\") / N >= per_thresh).collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07d689f7-f21f-422a-92b8-1cfe131c7c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Dropping columns {to_drop_missing} for more than {per_thresh * 100}% missing data\")\n",
    "telco_no_missing_df = telco_no_outliers_df.drop(*to_drop_missing)\n",
    "display(telco_no_missing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb0c4bb1-f5fa-492d-b0d9-a5e7d80e9783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Drop rows containing specific numbers of missing columns/fields**\n",
    "\n",
    "Rows with more than 80% the columns missing values are dropped using the **`na.drop()`** and the remaining missing values in numeric columns are imputed with 0, while missing values in string columns are imputed with 'N/A'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ed2aefc-4a67-4fc6-a156-2f1876ac6f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_cols = len(telco_no_missing_df.columns)\n",
    "telco_no_missing_df = telco_no_missing_df.na.drop(how='any', thresh=round(n_cols*.80)) # Drop rows where number of non-null values is below threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60066913-71f3-4d17-a826-3b6802db4908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count/Compare datasets before/after removing missing\n",
    "print(f\"Count - Before: {telco_no_outliers_df.count()} / After: {telco_no_missing_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a883bd3d-6456-4b69-b2be-c50cb3ad5f3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Impute Missing Data\n",
    "\n",
    "Replace missing values with a specified replacement value.\n",
    "\n",
    "* The **`num_cols`** and **`string_cols`** lists are created to identify numeric and string columns in the dataset, respectively.\n",
    "\n",
    "* Finally, missing values in the numeric and string columns are imputed with appropriate values using the **`na.fill()`**, resulting in the **`telco_imputed_df`** dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d384819d-1ef4-4c5e-b609-bc1e5d2a228b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Replace boolean missing with `False`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee8b61d-8cf9-4d81-80e8-d2010b8cb53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType\n",
    "\n",
    "\n",
    "# Get a list of boolean columns\n",
    "bool_cols = [c.name for c in telco_no_missing_df.schema.fields if (c.dataType == BooleanType())]\n",
    "\n",
    "# Impute\n",
    "telco_imputed_df = telco_no_missing_df.na.fill(value=False, subset=bool_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e7887a4-5ef5-401d-b5ce-cb22618b9b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Replace string missing with `No`**\n",
    "\n",
    "All string cols except `gender`, `Contract` and `PaymentMethod`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78527d11-4254-4618-960e-7e0312824227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Get list of string cols\n",
    "to_exclude = [\"customerID\", \"gender\", \"Contract\", \"PaymentMethod\"]\n",
    "string_cols = [c.name for c in telco_no_missing_df.drop(*to_exclude).schema.fields if c.dataType == StringType()]\n",
    "\n",
    "# Impute\n",
    "telco_imputed_df = telco_imputed_df.na.fill(value='No', subset=string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1f8e80d-955d-450d-8ed8-c2c01b8b5fe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare missing stats again\n",
    "calculate_missing(telco_imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79e889fc-d742-4d80-832b-522d68d885e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_imputed_df.write.mode(\"overwrite\").saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_imputed_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a22e7f7d-5754-45d5-aec8-97e1cb9cfd0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Encoding Categorical Features\n",
    "\n",
    "In this section, we will one-hot encode categorical/string features using Spark MLlib's `OneHotEncoder` estimator.\n",
    "\n",
    "If you are unfamiliar with one-hot encoding, there's a description below. If you're already familiar, you can skip ahead to the **One-hot encoding in Spark MLlib** section toward the bottom of the cell.\n",
    "\n",
    "#### Categorical features in machine learning\n",
    "\n",
    "Many machine learning algorithms are not able to accept categorical features as inputs. As a result, data scientists and machine learning engineers need to determine how to handle them. \n",
    "\n",
    "An easy solution would be remove the categorical features from the feature set. While this is quick, **you are removing potentially predictive information** &mdash; so this usually isn't the best strategy.\n",
    "\n",
    "Other options include ways to represent categorical features as numeric features. A few common options are:\n",
    "\n",
    "1. **One-hot encoding**: create dummy/binary variables for each category\n",
    "2. **Target/label encoding**: replace each category value with a value that represents the target variable (e.g. replace a specific category value with the mean of the target variable for rows with that category value)\n",
    "3. **Embeddings**: use/create a vector-representation of meaningful words in each category's value\n",
    "\n",
    "Each of these options can be really useful in different scenarios. We're going to focus on one-hot encoding here.\n",
    "\n",
    "#### One-hot encoding basics\n",
    "\n",
    "One-hot encoding creates a binary/dummy feature for each category in each categorical feature.\n",
    "\n",
    "In the example below, the feature **Animal** is split into three binary features &mdash; one for each value in **Animal**. Each binary feature's value is equal to 1 if its respective category value is present in **Animal** for each row. If its category value is not present in the row, the binary feature's value will be 0.\n",
    "\n",
    "![one-hot-encoding](../Includes/images/one-hot-encoding.png)\n",
    "\n",
    "#### One-hot encoding in Spark MLlib\n",
    "\n",
    "Even if you understand one-hot encoding, it's important to learn how to perform it using Spark MLlib.\n",
    "\n",
    "To one-hot encode categorical features in Spark MLlib, we are going to use two classes: [the **`StringIndexer`** class](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.StringIndexer.html#pyspark.ml.feature.StringIndexer) and [the **`OneHotEncoder`** class](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html#pyspark.ml.feature.OneHotEncoder).\n",
    "\n",
    "* The `StringIndexer` class indexes string-type columns to a numerical index. Each unique value in the string-type column is mapped to a unique integer.\n",
    "* The `OneHotEncoder` class accepts indexed columns and converts them to a one-hot encoded vector-type feature.\n",
    "\n",
    "#### Applying the `StringIndexer` -> `OneHotEncoder` -> `VectorAssembler`workflow\n",
    "\n",
    "First, we'll need to index the categorical features of the DataFrame. `StringIndexer` takes a few arguments:\n",
    "\n",
    "1. A list of categorical columns to index.\n",
    "2. A list names for the indexed columns being created.\n",
    "3. Directions for how to handle new categories when transforming data.\n",
    "\n",
    "Because `StringIndexer` has to learn which categories are present before indexing, it's an **estimator** &mdash; remember that means we need to call its `fit` method. Its result can then be used to transform our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "496e5aa5-7b0c-43a1-ab97-e8edea150575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample_df = telco_imputed_df.select(\"Contract\").distinct()\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f38398b2-713f-400a-9014-1c86a7833e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# StringIndexer\n",
    "string_cols = [\"Contract\"]\n",
    "index_cols = [column + \"_index\" for column in string_cols]\n",
    "\n",
    "string_indexer = StringIndexer(inputCols=string_cols, outputCols=index_cols, handleInvalid=\"skip\")\n",
    "string_indexer_model = string_indexer.fit(sample_df)\n",
    "indexed_df = string_indexer_model.transform(sample_df)\n",
    "\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5ef1322-cf2f-4210-ba98-85fb05592d66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once our data has been indexed, we are ready to use the `OneHotEncoder` estimator.\n",
    "\n",
    "<!-- <img src=\"https://files.training.databricks.com/images/icon_hint_24.png\"/>&nbsp; -->\n",
    "**\uD83D\uDCA1Hint:** Look at the [`OneHotEncoder` documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html#pyspark.ml.feature.OneHotEncoder) and our previous Spark MLlib workflows that use estimators for guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81c18e19-d57f-4fae-b7f0-9829be549328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "\n",
    "# Create a list of one-hot encoded feature names\n",
    "ohe_cols = [column + \"_ohe\" for column in string_cols]\n",
    "\n",
    "# Instantiate the OneHotEncoder with the column lists\n",
    "ohe = OneHotEncoder(inputCols=index_cols, outputCols=ohe_cols, handleInvalid=\"keep\")\n",
    "\n",
    "# Fit the OneHotEncoder on the indexed data\n",
    "ohe_model = ohe.fit(indexed_df)\n",
    "\n",
    "# Transform indexed_df using the ohe_model\n",
    "ohe_df = ohe_model.transform(indexed_df)\n",
    "ohe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a1e316-8eef-41ee-952b-aa5a151d5e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "selected_ohe_cols = [\"Contract_ohe\"]\n",
    "\n",
    "# Use VectorAssembler to assemble the selected one-hot encoded columns into a dense vector\n",
    "assembler = VectorAssembler(inputCols=selected_ohe_cols, outputCol=\"features\")\n",
    "result_df_dense = assembler.transform(ohe_df)\n",
    "\n",
    "# Select relevant columns for display\n",
    "result_df_display = result_df_dense.select(\"Contract\", \"features\")\n",
    "\n",
    "result_df_display.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "659e1540-bfaa-45b4-b463-86d1ce367c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Apply pre-existing embeddings to categorical/discrete features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60ce0774-d411-423e-ac0f-01d026acef16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's bin **`tenure`** to convert the discrete data into bins/categories format for further analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e04394-c1cf-4558-bc3f-a411795ff812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "column_to_bin = \"tenure\"\n",
    "display(telco_imputed_df.select(column_to_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3588dc93-cedb-452d-b02c-7725bc9de9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Specify bin ranges and column to bin\n",
    "bucketizer = Bucketizer(\n",
    "    splits=[0, 24, 48, float('Inf')],\n",
    "    inputCol=column_to_bin,\n",
    "    outputCol=f\"{column_to_bin}_bins\"\n",
    ")\n",
    "\n",
    "# Apply the bucketizer to the DataFrame\n",
    "bins_df = bucketizer.transform(telco_imputed_df.select(column_to_bin))\n",
    "\n",
    "# Recast bin numbers to integer\n",
    "bins_df = bins_df.withColumn(f\"{column_to_bin}_bins\", col(f\"{column_to_bin}_bins\").cast(\"integer\"))\n",
    "\n",
    "# Display the result\n",
    "display(bins_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1685780-23cb-40cf-85bf-49d606f5a12c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Map back to human-readable embedding scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a44b95b-08d4-49f7-8071-727cfee85f01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bins_embedded_df = (\n",
    "  bins_df.withColumn(f\"{column_to_bin}_embedded\", col(f\"{column_to_bin}_bins\").cast(StringType()))\n",
    "         .replace(to_replace = \n",
    "                  {\n",
    "                    \"0\":\"<2y\",\n",
    "                    \"1\":\"2-4y\",\n",
    "                    \"2\":\"<4y\"\n",
    "                  },\n",
    "                  subset=[f\"{column_to_bin}_embedded\"])\n",
    ")\n",
    "display(bins_embedded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "640f6a0b-ecbf-4c2e-a2b0-7f7869717829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Ordered Indexing\n",
    "\n",
    "Perform ordered indexing as an alternative categorical feature preparation for random forest modeling.\n",
    "\n",
    "Some categoricals are in fact `ordinal` and thus may require additional/manual encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6573f42-8044-4b4f-9736-a81faca9cab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ordinal_cat = \"Contract\"\n",
    "telco_imputed_df.select(ordinal_cat).distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec6ed752-6e75-4177-8d7a-270f6f05939c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Ordinal (category:index) map/dict\n",
    "ordered_list = [\n",
    "    \"Month-to-month\",\n",
    "    \"One year\",\n",
    "    \"Two year\"\n",
    "]\n",
    "\n",
    "ordinal_dict = {category: f\"{index+1}\" for index, category in enumerate(ordered_list)}\n",
    "display(ordinal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8488a0c-ec22-434a-a8b7-9e50d37dc9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column with ordered indexing\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "ordinal_df = (\n",
    "    telco_imputed_df\n",
    "    .withColumn(f\"{ordinal_cat}_ord\", col(ordinal_cat)) # Duplicate\n",
    "    .replace(to_replace=ordinal_dict, subset=[f\"{ordinal_cat}_ord\"]) # Map \n",
    "    .withColumn(f\"{ordinal_cat}_ord\", col(f\"{ordinal_cat}_ord\").cast('int')) # Cast to integer\n",
    ")\n",
    "\n",
    "display(ordinal_df.select(ordinal_cat, f\"{ordinal_cat}_ord\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da2021a8-9e61-45bf-91d8-c4c5d044ac54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Splitting Data (for Cross-Validation)\n",
    "\n",
    "Split modeling data into a train-test-holdout split as part of a modeling process\n",
    "\n",
    "In this section, we will perform the best-practice workflow for a train-test split using the Spark DataFrame API.\n",
    "\n",
    "Recall that due to things like changing cluster configurations and data partitioning, it can be difficult to ensure a reproducible train-test split. As a result, we recommend:\n",
    "\n",
    "1. Split the data using the **same random seed**\n",
    "2. Write out the train and test DataFrames\n",
    "\n",
    "<!-- <img src=\"https://files.training.databricks.com/images/icon_hint_24.png\"/>&nbsp; -->\n",
    "**\uD83D\uDCA1Hint:** Check out the [**`randomSplit`** documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.randomSplit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a530c0ff-fab6-4ca8-b56f-002617c98645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split with 80 percent of the data in train_df and 20 percent of the data in test_df\n",
    "train_df, test_df = telco_imputed_df.randomSplit([.8, .2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f967a8bd-5afe-417c-9bd8-6b0d0278b9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialize (OPTIONAL)\n",
    "train_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_customers_train\")\n",
    "test_df.write.mode(\"overwrite\").option(\"overwriteSchema\", True).saveAsTable(f\"{DA.catalog_name}.{DA.schema_name}.telco_customers_baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b673f99f-48b9-4c38-be83-59541eaad1cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Standardize Features in a Training Set\n",
    "\n",
    "For sake of example, we'll pick a column without missing data (e.g. `MonthlyCharges`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c581e80-e149-4a6b-9b73-077b5077682a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, RobustScaler, VectorAssembler\n",
    "\n",
    "\n",
    "num_cols_to_scale = [\"MonthlyCharges\"] # num_cols\n",
    "assembler = VectorAssembler().setInputCols(num_cols_to_scale).setOutputCol(\"numerical_assembled\")\n",
    "train_assembled_df = assembler.transform(train_df.select(*num_cols_to_scale))\n",
    "test_assembled_df = assembler.transform(test_df.select(*num_cols_to_scale))\n",
    "\n",
    "# Define scaler and fit on training set\n",
    "scaler = RobustScaler(inputCol=\"numerical_assembled\", outputCol=\"numerical_scaled\")\n",
    "scaler_fitted = scaler.fit(train_assembled_df)\n",
    "\n",
    "\n",
    "# Apply to both training and test set\n",
    "train_scaled_df = scaler_fitted.transform(train_assembled_df)\n",
    "test_scaled_df = scaler_fitted.transform(test_assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d525d4b-d0e1-4ac8-91e4-931732b03fb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Peek at Training set\")\n",
    "train_scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2714313-eab4-44f4-bfd4-621ec1273caa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Peek at Test set\")\n",
    "test_scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a060a82-32bd-4be6-a19d-f6ce7bea25d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Impute categorical missing values with the mode value using sparkml\n",
    "\n",
    "How to handle missing data only at training time and bake as part of inference pipeline to avoid data leakage and ensure that observation with missing data is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81e0d82c-044c-4191-98f3-257a319866b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_cols_to_impute = [\"PaymentMethod\"] # string_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be1a54b2-ea62-4ce0-9a26-cc10fe53d504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Index categoricals first as `Imputer` doesn't handle categoricals directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3188ec46-82c2-4d2c-9110-89ef14903774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Index categorical columns using StringIndexer\n",
    "cat_index_cols = [column + \"_index\" for column in categorical_cols_to_impute]\n",
    "cat_indexer = StringIndexer(\n",
    "    inputCols=categorical_cols_to_impute,\n",
    "    outputCols=cat_index_cols,\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Fit on training set\n",
    "cat_indexer_model = cat_indexer.fit(train_df.select(categorical_cols_to_impute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c65405cf-4ae7-4b19-9982-7305e97869c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform both train & test set using the fitted StringIndexer model\n",
    "cat_indexed_train_df = cat_indexer_model.transform(train_df.select(*categorical_cols_to_impute))\n",
    "cat_indexed_test_df = cat_indexer_model.transform(test_df.select(*categorical_cols_to_impute))\n",
    "# display(cat_indexed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498ce899-1f59-4882-839f-ebbfcc83bfbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_indexed_train_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d089e10b-b5a5-434b-a756-2ec326e6f782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The `StringIndexer` will create a new label (_e.g._ `4`) for missing when setting the `handleInvalid` flag to `keep` so it's important to keep track/revert indexes values back to `null` if we want to impute them, otherwise `null` will be treated as their own/separate category automatically.\n",
    "\n",
    "Alternatively for imputing categorical/strings, we can use `.fillna()` method by providing the `mode` value manually (as described above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce6dded3-fc75-4fde-9fbb-1591cdf43953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Revert indexes to `null` for missing categories\n",
    "for c in categorical_cols_to_impute:\n",
    "    cat_indexed_train_df = cat_indexed_train_df.withColumn(f\"{c}_index\", when(col(c).isNull(), None).otherwise(col(f\"{c}_index\")))\n",
    "    cat_indexed_test_df = cat_indexed_test_df.withColumn(f\"{c}_index\", when(col(c).isNull(), None).otherwise(col(f\"{c}_index\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc8bc70b-40f8-45ed-b887-42491492599f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_indexed_train_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac92f55-2340-4fc6-a353-d9a66fa65cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fit the imputer on indexed categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1c2a12-ec5a-4cea-aaf5-8b0f2622c7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "\n",
    "# Define 'mode' imputer\n",
    "output_cat_index_cols_imputed = [col+'_imputed' for col in cat_index_cols]\n",
    "mode_imputer = Imputer(\n",
    "  inputCols=cat_index_cols,\n",
    "  outputCols=output_cat_index_cols_imputed,\n",
    "  strategy=\"mode\"\n",
    "  )\n",
    "\n",
    "# Fit on training_df\n",
    "mode_imputer_fitted = mode_imputer.fit(cat_indexed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faed861e-6dcc-4b47-842e-4e31549e5c64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform both training & test sets\n",
    "cat_indexed_train_imputed_df = mode_imputer_fitted.transform(cat_indexed_train_df)\n",
    "cat_indexed_test_imputed_df  = mode_imputer_fitted.transform(cat_indexed_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2a54ead-0b26-4eff-a8e6-022db5818a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Peek at test set\n",
    "display(cat_indexed_test_imputed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d34ea41c-913d-4a7f-be33-73c8f14b4746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "This demo successfully provided a comprehensive understanding of data preparation for modeling and feature preparation, equipping you with the knowledge and skills to effectively prepare your data for modeling and analysis. We seamlessly saw how to correct data type, identifying and removing outliers, handling missing values through imputation or replacement, encoding categorical features, and standardizing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a3f33d-fc98-4557-8e70-79ccfbba2d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "2.1 Demo - Data Imputation and Transformation Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}